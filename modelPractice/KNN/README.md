#### KNN（k近邻算法）

k近邻算法假设给定一个数据集，其中的实例类别已定，分类时，对于新的实例，根据其k个最邻近的训练实例的类别，通过多数表决等方式进行预测。

k值的选择、距离的度量、分类决策规则是k近邻法的三个基本要素。

距离度量一般使用欧式距离，也可以选择其他距离，如Lp距离等。

k值的选择影响模型的近似误差与估计误差，k值较小时，近似误差较小，但是模型复杂度增大，容易发生过拟合；k值较大时，估计误差较小，但是与输入实例较远的数据点也会产生影响，所以近似误差会增大。

当使用k近邻去预测分类时，如果对所有训练数据进行线性扫描时，算法复杂度过高，计算成本太大，所以需要特殊的结构存储训练数据，如kd树等。

- 选取训练数据中位数作为root，以root的第一维作为划分标准，大于root的数据作为右子树，小于root的数据作为左子树
- 以左右子树根节点的第(j mod k) + 1维度作为划分标准，继续划分
- 递归执行1、2步，直至所有数据划分完毕
- 预测实例根据构建好的kdtree，寻找到它所在的划分叶结点
- 向上回溯，如果该点与当前最近点该点与父亲节点划分区域有交叉，则寻找该划分节点，比较距离是否更新
- 直至回溯至根节点

如果需要寻找k个最近邻，则需要借助队列或栈，将沿途查找路径节点记录。

kd-tree 实现：

为了改进kdtree在高维度下效率低下的情况，提出了ball-tree：
