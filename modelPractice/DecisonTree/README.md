#### 决策树

##### 基本算法框架

**纯度**：分支结点中所包含的样本尽可能属于同一类别。

##### 信息增益/互信息

“信息熵”是度量样本集合纯度最常用的一种指标。

假定集合D中第k类样本所占的比例为pk，则D的信息熵为。Ent越小，D的信息熵越低，纯度越高
$$
Ent(D) = \sum_{k=1}^{|y|}p_klog_2p_k
$$
“信息增益”，即使用某属性a对集合D进行划分时，纯度提升的程度，Dv代表根据属性a划分后的分支v样本集合的信息熵。
$$
Gain(D,a) = Ent(D) - \sum_v^{V}\frac{D^v}{D}Ent(D^v)
$$

##### ID3决策树

采用信息增益，对属性进行划分。

缺点：

- 无法处理连续值特征
- 无法处理缺失值
- 对类目多的属性有所偏好
- 过拟合问题

##### 增益率

信息增益对类目多的属性有所偏好，为了减轻这种偏好带来的不利影响，提出了“增益率”
$$
GainRatio(D,a) = \frac{Gain(D,a)}{IV(a)} \\
IV(a) = - \sum_{v=1}^{V}\frac{D^v}{D}log_2\frac{D^v}{D}
$$

##### C4.5决策树

该算法采用启发式，首先从候选属性集合中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

改进：

- 对连续特征进行离散化，对值首先进行排序，并将相邻两个样本值之间的平均值作为划分点，m个值，可以划分为m-1个特征点，计算每个点的增益率值，选择最高的，值大于该点的为类别1，否则为类别2

缺点：

- 

##### 基尼指数

$$
Gini(D) = 1 - \sum_{k=1}^{|y|}p_k^2 \\
属性a的基尼指数为：
GiniIndex(D,a) = \sum_{v}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$

基尼值越小，反映了该数据集D的纯度越高，基尼指数最小的，为最优划分属性。

##### CART

采用平方误差最小化/基尼指数最小化进行特征选择，生成二叉树。

改进：

- 在C4.5对连续值处理的基础上，将该属性在接下来的子结点中也参与特征选择

**回归树**：最小二乘回归树

一个很好的[实例](https://zhuanlan.zhihu.com/p/54355951)

**分类树：**基于基尼指数最小化

##### 剪枝操作

###### 预剪枝pre-puning

常用策略：

- 设定高度，决策树到达该高度时，停止生长
- 设定阈值，决策树到达某个节点，实例个数小于阈值，就停止生长
- 设定阈值，当决策树继续划分分支时，系统增益值与该阈值做对比，决定是否继续生长

缺陷：

- 存在欠拟合风险

###### 后剪枝

常用策略：

1. REP（Reduced-Error Puning，错误率降低剪枝）

   对决策树每个非叶结点子树更换为叶子结点，并用验证集计算更换之后在该验证集上面的错误率，如果错误率下降，则进行更换。

2. PEP（Pessimistic Error Puning，悲观剪枝）

   在C4.5决策树中使用。

   预知识：

   **二项概率分布**：
   $$
   E = np, \sigma^2 = npq
   $$
   **二项分布的正态逼近**：

   当np>=4，nq >=4时，二项分布近似于正态分布。

   ![](https://images0.cnblogs.com/blog2015/472792/201506/142006525192544.png)

   横坐标为成功次数，纵坐标为该次数对应的概率。

   在通过正态分布对二项分布进行近似的时候，需要进行[连续性修正](https://www.cnblogs.com/bigmonkey/p/12290228.html)以矫正结果。

   假设一个子树T，n(T)表示T的所有样本点，e(T)表示T中样本分错的样本数，则错误率r(T):
   $$
   r(T) = \frac{\sum_{s\in leaves}e(s)}{n(T)} \\
   分类错误视作二项分布，对其进行矫正：\\
   r(T) = \frac{\sum_{s\in leaves}(e(s) + 0.5)}{n(T)} = \frac{\sum_{s\in leaves}e(s) + 0.5*n(s)}{n(T)}
   $$

   则该树的误判次数均值和标准差可计算得到：
   $$
   E = N*r(T) \\
   var = \sqrt{N*r(T)*(1-r(T))}
   $$
   对该子树，换为叶结点，并得到其误判次数均值：
   $$
   E = N*e(T)
   $$
   自顶向下，如果原子树误判次数与更换为叶结点后到误判次数之差在一个给定的置信区间内，则不进行置换，负责置换，通常取一个标准差作为置信区间。
   $$
   E(subtree) - var(subtree) > E(leaf) => 进行剪枝
   $$
   一个计算的[实例](https://images2015.cnblogs.com/blog/870243/201608/870243-20160820210840656-940357399.png)

3. CCP（Cost-Complexity Puning，代价复杂度剪枝）

   在CART中被使用。

   设定两种值，代价和复杂度，以及对代价和复杂度关系进行衡量的参数a。

   代价的意义是对子树进行叶结点替换后增加的错分样本。

   复杂度的意义是进行叶结点替换后减少的叶结点数。
   $$
   cost = R(T_{leaf}) - R(T) \\
   complexity  = |Ni| - 1 \\
   a = \frac{cost}{complexity}
   $$

   - 基于自底向上，对原始决策树进行一系列修剪，得到一系列的树{T0,T1,....,Tm}，其中，Tm中只有根结点，而T0则为原始决策树。
   - 对这一系列树进行评价，采用交叉验证法进行选择最优树

   CCP生成子树序列的时间复杂度为O(n2)

4. MEP（Minimum Error Puning，最小误差剪枝）

   采用自底向上的方法，对于每个非叶结点，计算其替换为叶结点时的误差Er(T)，以及其每个分支节点的误差Er(Tt)并加权求和，如果Er(T) < Er(Tt)，则进行剪枝。

   n表示T的总结点数目，c表示T中主类的数目，k为类数目
   $$
   E_r(T) = \frac{n(T) - c(T) + k - 1}{n(t) + k} \\
   E_r(T_t) = \sum_v^V\frac{n(v)}{n(T)}E_r(v)
   $$

   | 比较项/剪枝方法  | REP      | PEP        | CCP              | MEP      |
   | :--------------: | -------- | ---------- | ---------------- | -------- |
   | 需要额外验证集？ | 1        | 0          | 交叉验证法不需要 | 0        |
   |     剪枝方式     | 自底向上 | 自顶向下   | 自底向上         | 自底向上 |
   |     误差估计     | 验证集   | 连续性校正 | cv               | 误差率   |
   |    时间复杂度    | O(n)     | O(n)       | O(n2)            | O(n)     |

该部分参考内容：

[谢小娇包教包会决策树之决策树剪枝](https://zhuanlan.zhihu.com/p/30296061)

[悲观简直算法](https://www.cnblogs.com/mdumpling/p/8087427.html)

[决策树-剪枝算法（二）](cnblogs.com/starfire86/p/5749334.html )

[概率统计17——点估计和连续性修正](https://www.cnblogs.com/bigmonkey/p/12290228.html)

#### 