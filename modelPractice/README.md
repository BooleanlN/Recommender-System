### Machine Learning
#### 基础概念

##### 模型分类

- 生成模型
- 判别模型

##### 期望风险、经验风险、结构风险

##### 正则化

##### 信息论

熵是关于不确定性的一个数学描述，熵起源于热力学，被视作无序分子运动紊乱程度的一种度量。

香农将熵的概念扩充至信息论的领域，用于刻画事件的不确定性程度。
$$
H(x) = -\sum_i^Np_i(x)log_2{p_i(x)}
$$
**信息熵**具备三条性质：

- 单调性。概率越高的事件，携带信息熵越低
- 非负性。
- 累加性。多个独立的随机事件同时发生的总不确定性，可以为各事件不确定性之和。

如果两个事件不独立，信息熵计算：
$$
H(A,B) = H(A) + H(B) - I(A,B) ,\ I(A,B)为互信息
$$
**交叉熵**

在训练模型时，目标是为了尽可能的接近真实的数据分布，真实分布下，消除事件不确定性付出的信息量是最少的，而交叉熵是为了衡量模型训练拟合的分布消除事件所付出的信息量的大小。

交叉熵越小，表示越接近真实分布。
$$
H(p,q) = -\sum_i^Np_i(x)log_2q_i(x) \\
$$
**相对熵/KL散度**

相对熵则是用来衡量真实分布与拟合分布之间付出信息量差异。最小化KL散度，是机器学习的目标。
$$
D_{KL}(p||q) = H(p,q) - H(p)
$$

##### 损失函数

分类模型：

**交叉熵(Cross-Entropy)：**
$$
loss = -\sum_i^Np_i(x)log_2q_i(x)
$$


**铰链损失(Hinge Loss)：**
$$
loss = \sum_imax(0,1 - y_i * f(x_i))
$$
Hinge loss 损失更易于计算，梯度下降计算速度更快（很多梯度为0）

cross entropy 准确率更高

回归模型：

**平均平方损失(MSE)：**
$$
loss = \frac{1}{N}\sum_i^N(f_i - y_i)^2
$$
**平均绝对值损失(MAE)：**
$$
loss = \frac{1}{N}\sum_i^N|f_i - y_i|
$$
MSE相比于MAE，对偏差大的惩罚力度更大，因此对异常值更加敏感，MAE鲁棒性更好。

**Huber损失：**
$$
L = \left\{
\begin{array}{**lr**}
\frac{1}{2}(y-f(x))^2, \ for|y-f(x)| \le \delta, \\
\delta|y-f(x)|-\frac{1}{2}\delta^2,  \ otherwise
\end{array}
\right.
$$

#### 